from dataclasses import dataclass, field
from typing import List, Optional
from pathlib import Path
from tqdm import tqdm
import time

from mlx.utils import tree_flatten
import mlx.core as mx
import mlx.nn as nn
import numpy as np

from mlx_lm.tuner.callbacks import TrainingCallback

from .sft_trainer import SFTTrainingArgs, average_gradients, grad_checkpoint

from mlx_lm.models import cache
from mlx_lm.generate import generate, make_sampler
from .grpo_reward_functions import (
    RewardFunctions,
    r1_accuracy_reward_func,
    r1_count_xml,
    r1_int_reward_func,
    r1_soft_format_reward_func,
    r1_strict_format_reward_func,
)


@dataclass
class GRPOTrainingArgs(SFTTrainingArgs):
    group_size: int = field(
        default=4,
        metadata={"help": "Number of responses per prompt."},
    )
    beta: float = field(default=0.1, metadata={"help": "KL penalty coefficient."})
    epsilon: float = field(
        default=1e-4, metadata={"help": "The Epsilon for numerical stability."}
    )
    epsilon_high: float = field(
        default=None, metadata={"help": "For DAPO Upper-bound epsilon value for clipping. If not specified, it defaults to the same value as the lower-bound specified in argument epsilon."}
    )
    max_completion_length: int = field(
        default=512, metadata={"help": "Number of Generations."}
    )
    reference_model_path: str = field(
        default=None,
        metadata={
            "help": "Path to reference model weights. If None, uses the same model."
        },
    )
    temperature: float = field(
        default=0.8,
        metadata={
            "help": "Temperature for sampling. The higher the temperature, the more random the completions."
        },
    )
    grpo_loss_type: str = field(
        default="grpo",
        metadata={
            "help": "Type of loss to use for GRPO. Supported: 'grpo', 'bnpo', 'dr_grpo'."
        }
    )
    reward_weights: Optional[List[float]] = field(
        default=None,
        metadata={
            "help": "Weights for each reward function. Must match the number of reward functions. If `None`, all rewards are weighted equally with weight `1.0`."
        },
    )
    importance_sampling_level: str = field(
        default=None,
        metadata={
            "help": "importance_sampling_level (`str`, *optional*, defaults to None): "
                "Controls whether importance sampling ratios are computed at the 'token' or 'sequence' level. "
                "keeps the raw per-token log-probability ratios (one weight per token).  'sequence' averages the "
                "log-probability ratios across valid tokens to produce a single ratio per sequence. The "
                "GSPO paper https://huggingface.co/papers/2507.18071) shows that sequence-level sampling often yields more "
                "stable training and better alignment with sequence-level rewards."
        },
    )
    use_answer_tags: bool = field(
        default=False,
        metadata={
            "help": "Whether to enforce answer tags in generation. If False, answer tags are optional."
        },
    )


def get_per_token_logps(model: nn.Module, inputs, lengths):
    logits = model(inputs).astype(mx.float16)
    logits = logits[:, :-1, :]
    targets = inputs[:, 1:]
    per_token_logps = []
    for i in range(logits.shape[0]):
        seq_len = int(lengths[i]) - 1
        seq_logits = logits[i, :seq_len]
        seq_targets = targets[i, :seq_len]
        log_probs = nn.log_softmax(seq_logits, axis=-1)
        token_log_probs = mx.take_along_axis(
            log_probs, seq_targets.reshape(seq_len, 1), axis=-1
        ).squeeze(-1)
        per_token_logps.append(token_log_probs)
    mx.eval(logits)
    return per_token_logps

def generate_grpo(
    model: nn.Module,
    tokenizer,
    prompt_tokens,
    max_tokens: int,
    group_size: int,
    temperature: float,
    batch_size: int,
    end_token: Optional[str] = None,  # Changed to Optional with default None
    stop_at_think_close: bool = True  # Option to stop at </think> instead
):
    """
    Generate completions for GRPO training.

    Args:
        model: The language model
        tokenizer: The tokenizer
        prompt_tokens: List of tokenized prompts
        max_tokens: Maximum number of tokens to generate
        group_size: Number of completions per prompt
        temperature: Sampling temperature
        batch_size: Batch size for generation
        end_token: Optional end token to stop generation (e.g., "</answer>")
        stop_at_think_close: If True and no end_token, stop at "</think>" + some content
    """
    model.eval()
    all_completions = []
    all_completion_texts = []
    batch_indices = []

    total_samples = len(prompt_tokens)

    for i in range(0, total_samples, batch_size):
        current_batch_size = min(batch_size, total_samples - i)
        batch_prompts = prompt_tokens[i : i + current_batch_size]

        for j, prompt in enumerate(batch_prompts):
            for k in range(group_size):
                prompt_text = tokenizer.decode(prompt)
                sampler = make_sampler(
                    temperature,
                    top_p=1.0,
                    min_p=0.0,
                    min_tokens_to_keep=1,
                    top_k=0,
                    xtc_probability=0.0,
                    xtc_threshold=0.0,
                    xtc_special_tokens=tokenizer.encode("\n") + list(tokenizer.eos_token_ids),
                )

                prompt_cache = cache.make_prompt_cache(model)
                completion = generate(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=prompt_text,
                    max_tokens=max_tokens,
                    verbose=False,
                    sampler=sampler,
                    prompt_cache=prompt_cache,
                )

                if isinstance(completion, str):
                    completion_ids = tokenizer.encode(completion)
                    completion_text = completion
                else:
                    completion_ids = completion
                    completion_text = tokenizer.decode(completion_ids)

                # Handle different stopping conditions
                if end_token:
                    # If a specific end token is provided, use it
                    end_sequence = tokenizer.encode(end_token)
                    if len(completion_ids) >= len(end_sequence):
                        # Check if the completion ends with the end_token
                        if completion_ids[-len(end_sequence):] == end_sequence:
                            completion_ids = completion_ids[:-len(end_sequence)]
                elif stop_at_think_close and "</think>" in completion_text:
                    # If no end_token but we want to stop after </think> content
                    # Find the position after </think> and keep some content after it
                    think_close_pos = completion_text.find("</think>")
                    if think_close_pos != -1:
                        # Keep content up to </think> plus a reasonable amount after
                        # This ensures we have the answer content after </think>
                        after_think = completion_text[think_close_pos + 8:]  # 8 is len("</think>")

                        # If there's content after </think>, keep it
                        # But stop at natural boundaries like double newlines or answer tags if present
                        if after_think:
                            # Look for natural stopping points
                            natural_stops = ["\n\n", "<answer>", "</answer>"]
                            min_stop = len(after_think)

                            for stop in natural_stops:
                                pos = after_think.find(stop)
                                if pos != -1 and pos < min_stop:
                                    min_stop = pos

                            # If we found </answer>, include it
                            if "</answer>" in after_think[:min_stop + 10]:
                                answer_end = after_think.find("</answer>")
                                if answer_end != -1:
                                    min_stop = answer_end + 9  # Include </answer>

                            truncated_completion = completion_text[:think_close_pos + 8 + min_stop]
                            completion_ids = tokenizer.encode(truncated_completion)

                completion_ids = mx.array(completion_ids)
                all_completions.append(mx.stop_gradient(completion_ids))
                all_completion_texts.append(completion_text)
                batch_indices.append(i + j)

    mx.clear_cache()
    return all_completions, all_completion_texts, batch_indices


def grpo_loss(
    model,
    ref_model,
    tokenizer,
    batch,
    completions=None,
    completion_texts=None,
    batch_indices=None,
    reward_funcs: Optional[List[RewardFunctions]] = None,
    beta: float = 0.1,
    group_size: int = 4,
    epsilon: float = 1e-4,
    epsilon_high: float = None,
    max_tokens: int = 64,
    temperature: float = 0.8,
    reward_weights: Optional[List[float]] = None,
    batch_size: int = 1,
    importance_sampling_level: str = "token",
    grpo_loss_type: str = "grpo",
    use_answer_tags: bool = False,  # New parameter to control answer tag usage
):
    prompt_tokens, _, prompt_text, answer_text, type_info = batch

    if (
        completions is not None
        and completion_texts is not None
        and batch_indices is not None
    ):
        all_completions = completions
        all_completion_texts = completion_texts
        batch_indices = batch_indices
    else:
        # Determine end_token based on use_answer_tags parameter
        end_token = "</answer>" if use_answer_tags else None

        all_completions, all_completion_texts, batch_indices = generate_grpo(
            model=model,
            tokenizer=tokenizer,
            prompt_tokens=prompt_tokens,
            max_tokens=max_tokens,
            group_size=group_size,
            temperature=temperature,
            batch_size=batch_size,
            end_token=end_token,
            stop_at_think_close=not use_answer_tags,  # Stop at </think> if not using answer tags
        )

    if not all_completions:
        raise ValueError(
            "No completions were generated. Please check your model and inputs."
        )

    expanded_answers = []
    expanded_prompts = []
    expanded_types = []
    unique_prompt_indices = sorted(set(batch_indices))
    grouped_completions = {idx: [] for idx in unique_prompt_indices}

    for i, completion_idx in enumerate(batch_indices):
        grouped_completions[completion_idx].append(i)

    ordered_completions = []
    ordered_completion_texts = []
    ordered_batch_indices = []

    for prompt_idx in unique_prompt_indices:
        completion_indices = grouped_completions[prompt_idx]
        for idx in completion_indices:
            ordered_completions.append(all_completions[idx])
            ordered_completion_texts.append(all_completion_texts[idx])
            ordered_batch_indices.append(batch_indices[idx])
            expanded_answers.append(answer_text[prompt_idx])
            expanded_prompts.append(prompt_text[prompt_idx])
            expanded_types.append(type_info[prompt_idx] if type_info else None)

    if not reward_funcs:
        reward_funcs = [
            r1_accuracy_reward_func,
            r1_int_reward_func,
            r1_strict_format_reward_func,
            r1_soft_format_reward_func,
            r1_count_xml,
        ]

    if reward_weights is None:
        reward_weights = [1.0] * len(reward_funcs)
    elif len(reward_weights) != len(reward_funcs):
        raise ValueError(
            f"reward_weights must have same length as reward_funcs. "
            f"Got {len(reward_weights)} weights for {len(reward_funcs)} functions."
        )

    all_rewards = []
    reward_details = {func.__name__: [] for func in reward_funcs}

    for i, reward_func in enumerate(reward_funcs):
        rewards = reward_func(
            expanded_prompts, ordered_completion_texts, expanded_answers, expanded_types
        )
        weighted_rewards = [r * reward_weights[i] for r in rewards]
        all_rewards.append(weighted_rewards)
        reward_details[reward_func.__name__] = rewards

    total_rewards = [
        sum(all_rewards[j][i] for j in range(len(reward_funcs)))
        for i in range(len(ordered_completion_texts))
    ]

    grouped_rewards = []
    for prompt_idx in unique_prompt_indices:
        completion_indices = grouped_completions[prompt_idx]
        prompt_rewards = [total_rewards[idx] for idx in completion_indices]
        grouped_rewards.append(prompt_rewards)

    model.train()
    if ref_model is not None:
        ref_model.eval()

    all_inputs = []
    all_lengths = []
    for prompt, completion in zip(prompt_tokens, ordered_completions):
        inputs = mx.concatenate([prompt, completion])
        all_inputs.append(inputs)
        all_lengths.append(len(inputs))

    max_len = max(all_lengths)
    padded_inputs = []
    for inputs, length in zip(all_inputs, all_lengths):
        padded = mx.pad(inputs, [(0, max_len - length)], constant_values=tokenizer.pad_token_id)
        padded_inputs.append(padded)

    batch_inputs = mx.stack(padded_inputs)
    batch_lengths = mx.array(all_lengths)

    policy_per_token_logps = get_per_token_logps(model, batch_inputs, batch_lengths)

    if ref_model is not None:
        with mx.no_grad():
            ref_per_token_logps = get_per_token_logps(ref_model, batch_inputs, batch_lengths)
    else:
        ref_per_token_logps = policy_per_token_logps

    prompt_token_counts = [len(p) for p in prompt_tokens]

    all_losses = mx.array(0.0)
    ntokens = 0
    policy_logps = []
    ref_logps = []
    kls = []
    loss_masks = []
    ratios = []
    clipped_low_count = 0
    clipped_high_count = 0
    total_comparisons = 0

    for i, (policy_logp, ref_logp, prompt_len) in enumerate(
        zip(policy_per_token_logps, ref_per_token_logps, prompt_token_counts)
    ):
        completion_logp = policy_logp[prompt_len:]
        ref_completion_logp = ref_logp[prompt_len:]

        if len(completion_logp) == 0:
            continue

        if importance_sampling_level == "sequence":
            policy_logp_sum = mx.sum(completion_logp)
            ref_logp_sum = mx.sum(ref_completion_logp)
            ratio = mx.exp(policy_logp_sum - ref_logp_sum)
            ratios.append(ratio)
        else:
            ratio = mx.exp(completion_logp - ref_completion_logp)
            ratios.extend([r.item() for r in ratio])

        policy_logps.append(mx.sum(completion_logp))
        ref_logps.append(mx.sum(ref_completion_logp))
        kl = mx.sum(ref_completion_logp - completion_logp)
        kls.append(kl)

        loss_mask = mx.ones_like(completion_logp)
        loss_masks.append(loss_mask)
        ntokens += len(completion_logp)

    all_losses = mx.array(0.0)

    for prompt_idx, prompt_rewards in enumerate(grouped_rewards):
        if len(prompt_rewards) == 0:
            continue

        baseline = np.mean(prompt_rewards)
        advantages = [r - baseline for r in prompt_rewards]

        start_idx = prompt_idx * group_size
        end_idx = start_idx + len(prompt_rewards)

        for i, advantage in enumerate(advantages):
            idx = start_idx + i
            if idx >= len(policy_logps):
                break

            kl_penalty = kls[idx] if idx < len(kls) else mx.array(0.0)

            if grpo_loss_type == "grpo":
                if importance_sampling_level == "sequence":
                    ratio = ratios[idx]
                    epsilon_high_value = epsilon_high if epsilon_high is not None else epsilon
                    clipped_ratio = mx.clip(ratio, 1.0 - epsilon, 1.0 + epsilon_high_value)

                    if ratio < (1.0 - epsilon):
                        clipped_low_count += 1
                    if ratio > (1.0 + epsilon_high_value):
                        clipped_high_count += 1
                    total_comparisons += 1

                    loss = -clipped_ratio * advantage * policy_logps[idx]
                else:
                    loss = -advantage * policy_logps[idx]
            elif grpo_loss_type == "bnpo":
                loss = -advantage * policy_logps[idx] - beta * ref_logps[idx]
            elif grpo_loss_type == "dr_grpo":
                dr_factor = mx.minimum(
                    1.0,
                    mx.sqrt(mx.exp(policy_logps[idx] - ref_logps[idx]))
                )
                loss = -dr_factor * advantage * policy_logps[idx]
            else:
                raise ValueError(f"Unknown GRPO loss type: {grpo_loss_type}")

            loss = loss + beta * kl_penalty
            all_losses = all_losses + loss

    avg_loss = all_losses / ntokens if ntokens > 0 else mx.array(0.0)

    # Calculate metrics
    total_rewards_mean = np.mean(total_rewards) if total_rewards else 0
    total_rewards_std = np.std(total_rewards) if total_rewards else 0
    grouped_rewards_flat = [r for group in grouped_rewards for r in group]
    grouped_rewards_mean = np.mean(grouped_rewards_flat) if grouped_rewards_flat else 0
    grouped_rewards_std = np.std(grouped_rewards_flat) if grouped_rewards_flat else 0
    kl_mean = np.mean([k.item() for k in kls]) if kls else 0

    average_generated_tokens = np.mean([
        len(completion) - prompt_token_counts[i % len(prompt_token_counts)]
        for i, completion in enumerate(all_inputs)
    ]) if all_inputs else 0

    clip_ratio_low = clipped_low_count / max(total_comparisons, 1)
    clip_ratio_high = clipped_high_count / max(total_comparisons, 1)
    clip_ratio_total = (clipped_low_count + clipped_high_count) / max(total_comparisons, 1)

    metrics = {
        "total_rewards_mean": total_rewards_mean,
        "total_rewards_std": total_rewards_std,
        "grouped_rewards_mean": grouped_rewards_mean,
        "grouped_rewards_std": grouped_rewards_std,
        "kl": kl_mean,
        "average_generated_tokens": average_generated_tokens,
        "clip_ratio_low": clip_ratio_low,
        "clip_ratio_high": clip_ratio_high,
        "clip_ratio_total": clip_ratio_total,
    }

    # Add per-reward function metrics
    for func_name, rewards in reward_details.items():
        if rewards:
            metrics[f"{func_name}_mean"] = np.mean(rewards)
            metrics[f"{func_name}_std"] = np.std(rewards)
            metrics[f"{func_name}_coverage"] = np.mean([1 if r > 0 else 0 for r in rewards])

    return avg_loss, ntokens, metrics




def iterate_grpo_batches(dataset, batch_size, max_seq_length, train=False):
    has_types = isinstance(dataset[0], tuple) and len(dataset[0]) == 5

    if not dataset or not isinstance(dataset[0], tuple) or (not has_types and len(dataset[0]) != 4):
        raise ValueError(
            "Dataset must be list of (prompt_tokens, answer_tokens, prompt_str, answer_str[, type]) tuples"
        )

    def length_key(i):
        return len(dataset[i][0]) + len(dataset[i][1])

    idx = sorted(range(len(dataset)), key=length_key)

    if len(dataset) < batch_size:
        raise ValueError(
            f"Dataset must have at least batch_size={batch_size} "
            f"examples but only has {len(dataset)}."
        )

    step = mx.distributed.init().size()
    if batch_size % step != 0:
        raise ValueError("The batch size must be divisible by the number of workers")

    def batch_index_generator():
        for i in range(0, len(idx) - batch_size + 1, batch_size):
            yield idx[i : i + batch_size : step]

    while True:
        indices = (
            np.random.permutation(list(batch_index_generator()))
            if train
            else batch_index_generator()
        )

        for batch_idx in indices:
            current_batch = [dataset[j] for j in batch_idx]

            prompts_tokens = [item[0] for item in current_batch]
            answers_tokens = [item[1] for item in current_batch]
            prompts_text = [item[2] for item in current_batch]
            answers_text = [item[3] for item in current_batch]
            types = [item[4] for item in current_batch] if has_types else None

            yield prompts_tokens, answers_tokens, prompts_text, answers_text, types

        if not train:
            break





def evaluate_grpo(
    model,
    dataset,
    loss_fn,
    ref_model,
    reward_funcs,
    tokenizer,
    group_size,
    batch_size,
    num_batches,
    max_seq_length,
    max_tokens,
    beta,
    epsilon,
    epsilon_high,
    temperature,
    iterate_batches,
    grpo_loss_type="grpo",
    use_answer_tags=False,
):
    model.eval()
    all_losses = mx.array(0.0)
    ntokens = 0
    accumulated_metrics = {}

    for _ in range(num_batches):
        batch = next(iterate_batches(
            dataset=dataset,
            batch_size=batch_size,
            max_seq_length=max_seq_length,
            train=False,
        ))

        loss, toks, metrics = loss_fn(
            model=model,
            ref_model=ref_model,
            tokenizer=tokenizer,
            batch=batch,
            reward_funcs=reward_funcs,
            beta=beta,
            group_size=group_size,
            epsilon=epsilon,
            epsilon_high=epsilon_high,
            max_tokens=max_tokens,
            temperature=temperature,
            batch_size=batch_size,
            grpo_loss_type=grpo_loss_type,
            use_answer_tags=use_answer_tags,
        )

        all_losses += loss * toks
        ntokens += toks

        for k, v in metrics.items():
            if k not in accumulated_metrics:
                accumulated_metrics[k] = 0
            accumulated_metrics[k] += v

    avg_metrics = {k: v / num_batches for k, v in accumulated_metrics.items()}
    avg_loss = (all_losses / ntokens).item()

    return avg_loss, ntokens, avg_metrics


def train_grpo(
    model: nn.Module,
    ref_model: Optional[nn.Module],
    tokenizer,
    optimizer,
    train_dataset,
    val_dataset,
    reward_funcs: Optional[List[RewardFunctions]] = None,
    args: GRPOTrainingArgs = GRPOTrainingArgs(),
    loss_fn: callable = grpo_loss,
    iterate_batches: callable = iterate_grpo_batches,
    training_callback: TrainingCallback = None,
):
    """
    Train a model using GRPO with optional answer tags.
    """
    mx.set_wired_limit(mx.metal.device_info()["max_recommended_working_set_size"])
    tqdm.write(f"Starting training..., iters: {args.iters}")
    world = mx.distributed.init()
    world_size = world.size()
    rank = world.rank()
    if world_size > 1:
        tqdm.write(f"Node {rank} of {world_size}")

    if args.grad_checkpoint:
        grad_checkpoint(model.layers[0])

    # Use default reward functions if none provided
    if reward_funcs is None:
        reward_funcs = [
            r1_accuracy_reward_func,
            r1_int_reward_func,
            r1_strict_format_reward_func,
            r1_soft_format_reward_func,
            r1_count_xml,
        ]

    state = [model.state, optimizer.state]

    def step(batch):
        prompt_tokens, targets, prompt_lens, target_lens, type_info = batch

        # Determine end_token based on use_answer_tags setting
        end_token = "</answer>" if args.use_answer_tags else None

        all_completions, all_completion_texts, batch_indices = generate_grpo(
            model=model,
            tokenizer=tokenizer,
            prompt_tokens=prompt_tokens,
            max_tokens=args.max_completion_length,
            group_size=args.group_size,
            temperature=args.temperature,
            batch_size=args.batch_size,
            end_token=end_token,
            stop_at_think_close=not args.use_answer_tags,
        )

        mx.clear_cache()

        (lvalue, toks, metrics), grad = loss_value_and_grad(
            model,
            tokenizer=tokenizer,
            batch=(prompt_tokens, targets, prompt_lens, target_lens, type_info),
            completions=all_completions,
            completion_texts=all_completion_texts,
            batch_indices=batch_indices,
            reward_funcs=reward_funcs,
            beta=args.beta,
            group_size=args.group_size,
            epsilon=args.epsilon,
            epsilon_high=args.epsilon_high,
            ref_model=ref_model,
            grpo_loss_type=args.grpo_loss_type,
            max_tokens=args.max_completion_length,
            importance_sampling_level=args.importance_sampling_level,
            use_answer_tags=args.use_answer_tags,
        )

        if (it + 1) % args.gradient_accumulation_steps == 0:
            grad = average_gradients(grad)
            optimizer.update(model, grad)

        return (lvalue / args.gradient_accumulation_steps), toks, metrics

    loss_value_and_grad = nn.value_and_grad(model, loss_fn)

    losses = 0
    n_tokens = 0
    steps = 0
    trained_tokens = 0
    accumulated_metrics = {
        "total_rewards_mean": 0,
        "total_rewards_std": 0,
        "grouped_rewards_mean": 0,
        "grouped_rewards_std": 0,
        "kl": 0,
        'average_generated_tokens': 0,
        "clip_ratio_low": 0,
        "clip_ratio_high": 0,
        "clip_ratio_total": 0,
    }
    for reward_func in reward_funcs:
        func_name = reward_func.__name__
        accumulated_metrics[f"{func_name}_mean"] = 0
        accumulated_metrics[f"{func_name}_std"] = 0
        accumulated_metrics[f"{func_name}_coverage"] = 0

    start = time.perf_counter()
    pbar = tqdm(range(1, args.iters + 1), desc="Training", disable=rank != 0)
    for it in pbar:
        batch = next(iterate_batches(
            dataset=train_dataset,
            batch_size=args.batch_size,
            max_seq_length=args.max_seq_length,
            train=True,
        ))

        if it == 1 or it % args.steps_per_eval == 0 or it == args.iters:
            stop = time.perf_counter()
            val_loss, val_ntokens, val_metrics = evaluate_grpo(
                model=model,
                dataset=val_dataset,
                loss_fn=loss_fn,
                ref_model=ref_model,
                reward_funcs=reward_funcs,
                tokenizer=tokenizer,
                group_size=args.group_size,
                batch_size=args.batch_size,
                num_batches=args.val_batches,
                max_seq_length=args.max_seq_length,
                max_tokens=args.max_completion_length,
                beta=args.beta,
                epsilon=args.epsilon,
                epsilon_high=args.epsilon_high,
                temperature=args.temperature,
                iterate_batches=iterate_batches,
                grpo_loss_type=args.grpo_loss_type,
                use_answer_tags=args.use_answer_tags,
            )
            val_time = time.perf_counter() - stop
            if rank == 0:
                tqdm.write(
                    f"Iter {it}: "
                    f"Val loss {val_loss:.3f}, "
                    f"Val took {val_time:.3f}s"
                )

            if training_callback is not None:
                val_info = {
                    "iteration": it,
                    "val_loss": val_loss,
                    "val_time": val_time,
                }
                training_callback.on_val_loss_report(val_info)

            start = time.perf_counter()

        lvalue, toks, metrics = step(batch)
        losses += lvalue
        n_tokens += toks
        steps += 1

        for k, v in metrics.items():
            accumulated_metrics[k] += v

        mx.eval(state, losses, n_tokens)

        if it % args.steps_per_report == 0 or it == args.iters:
            stop = time.perf_counter()

            train_loss = mx.distributed.all_sum(losses).item() / (steps * world_size)
            avg_metrics = {
                k: v / (steps * world_size) for k, v in accumulated_metrics.items()
            }
            n_tokens = mx.distributed.all_sum(n_tokens).item()
            learning_rate = optimizer.learning_rate.item()
            it_sec = args.steps_per_report / (stop - start)
            tokens_sec = float(n_tokens) / (stop - start)
            trained_tokens += n_tokens
            peak_mem = mx.get_peak_memory() / 1e9

            if rank == 0:
                pbar.set_postfix({
                    'loss': f"{train_loss:.3f}",
                    'it/s': f"{it_sec:.3f}",
                })
                tqdm.write(
                    f"\nIter {it}: "
                    f"loss {train_loss:.3f}, "
                    f"total_r_mean {avg_metrics['total_rewards_mean']:.3f}, "
                    f"total_r_std {avg_metrics['total_rewards_std']:.3f}, "
                    f"group_r_mean {avg_metrics['grouped_rewards_mean']:.3f}, "
                    f"group_r_std {avg_metrics['grouped_rewards_std']:.3f}, "
                    f"kl {avg_metrics['kl']:.3f}, "
                    f"lr {learning_rate:.3e}, "
                    f"it/s {it_sec:.3f}, "
                    f"tok/s {tokens_sec:.3f}, "
                    f"peak_mem {peak_mem:.3f}GB"
                )

            if training_callback is not None:
                train_info = {
                    "iteration": it,
                    "train_loss": train_loss,
                    **{f"train_{k}": v for k, v in avg_metrics.items()},
                    "learning_rate": learning_rate,
                    "iterations_per_second": it_sec,
                    "tokens_per_second": tokens_sec,
                    "trained_tokens": trained_tokens,
                    "peak_memory": peak_mem,
                }
                training_callback.on_train_loss_report(train_info)

            losses = 0
            n_tokens = 0
            steps = 0
            accumulated_metrics = {k: 0 for k in accumulated_metrics}
            start = time.perf_counter()

        if it % args.steps_per_save == 0:
            adapter_weights = dict(tree_flatten(model.trainable_parameters()))
            mx.save_safetensors(str(args.adapter_file), adapter_weights)
            checkpoint = (
                Path(args.adapter_file).parent / f"{it:07d}_adapters.safetensors"
            )
            mx.save_safetensors(str(checkpoint), adapter_weights)
            tqdm.write(
                f"\n"
                f"Iter {it}: Saved adapter weights to "
                f"{args.adapter_file} and {checkpoint}."
            )

    adapter_weights = dict(tree_flatten(model.trainable_parameters()))
    mx.save_safetensors(str(args.adapter_file), adapter_weights)
    tqdm.write(f"Saved final weights to {args.adapter_file}.")
